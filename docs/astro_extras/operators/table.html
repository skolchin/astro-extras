<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>astro_extras.operators.table API documentation</title>
<meta name="description" content="Table operations" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>astro_extras.operators.table</code></h1>
</header>
<section id="section-intro">
<p>Table operations</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Astro SDK Extras project
# (c) kol, 2023

&#34;&#34;&#34; Table operations &#34;&#34;&#34;

import pandas as pd
from airflow.models.xcom_arg import XComArg
from airflow.models.dag import DagContext
from airflow.operators.generic_transfer import GenericTransfer
from airflow.utils.context import Context
from airflow.utils.task_group import TaskGroup
from airflow.exceptions import AirflowFailException

from astro import sql as aql
from astro.databases import create_database
from astro.sql.table import Table
from astro.databases.base import BaseDatabase
from astro.airflow.datasets import kwargs_with_datasets

from typing import Union, Literal, Optional, Iterable, List

from .session import ETLSession, ensure_session
from ..utils.utils import ensure_table, schedule_ops
from ..utils.template import get_template_file

class TableTransfer(GenericTransfer):
    &#34;&#34;&#34; Customized table transfer operator. Usually is used within `transfer_table` function &#34;&#34;&#34;

    template_fields = (&#34;sql&#34;, &#34;preoperator&#34;, &#34;source_table&#34;, &#34;destination_table&#34;)
    template_ext = (&#34;.sql&#34;, &#34;.hql&#34; )
    template_fields_renderers = {&#34;sql&#34;: &#34;sql&#34;, &#34;preoperator&#34;: &#34;sql&#34;}
    ui_color = &#34;#b0f07c&#34;

    def __init__(
        self,
        *,
        source_table: Table,
        destination_table: Table,
        mode: Optional[Literal[&#39;default&#39;, &#39;delta&#39;, &#39;full&#39;]] = &#39;default&#39;,
        session: Optional[ETLSession] = None,
        **kwargs,
    ) -&gt; None:

        source_db = create_database(source_table.conn_id, source_table)
        dest_db = create_database(destination_table.conn_id, destination_table)

        task_id = kwargs.pop(&#39;task_id&#39;, f&#39;transfer-{source_table.name}&#39;)
        sql = kwargs.pop(&#39;sql&#39;, self._get_sql(source_table, source_db, session))

        super().__init__(task_id=task_id,
                         sql=sql,
                         destination_table=dest_db.get_table_qualified_name(destination_table),
                         source_conn_id=source_table.conn_id,
                         destination_conn_id=destination_table.conn_id,
                         **kwargs)

        self.source = source_table
        self.source_table = source_db.get_table_qualified_name(source_table)
        self.destination = destination_table
        self.mode = mode
        self.session = session

    def _get_sql(self, table: Table, db: BaseDatabase, session: ETLSession) -&gt; str:
        &#34;&#34;&#34; Internal - get a sql statement or template for given table &#34;&#34;&#34;

        if (sql_file := get_template_file(table.name, &#39;.sql&#39;)):
            self.log.info(f&#39;Using template file {sql_file}&#39;)
            return sql_file

        full_name = db.get_table_qualified_name(table)
        if session:
            return &#39;select {{ti.xcom_pull(key=&#34;session&#34;).session_id}} as session_id, * from &#39; + full_name

        return &#39;select * from &#39; + full_name

    def execute(self, context: Context):
        self.session = ensure_session(self.session, context)
        if self.session:
            if not self.source_conn_id:
                self.source_conn_id = self.session.source_conn_id
            if not self.destination_conn_id:
                self.destination_conn_id = self.session.destination_conn_id

        if not self.source_conn_id:
            raise AirflowFailException(&#39;source connection not specified&#39;)
        if not self.destination_conn_id:
            raise AirflowFailException(&#39;destination connection not specified&#39;)
        if self.source_conn_id == self.destination_conn_id and self.source_table == self.destination_table:
            raise AirflowFailException(&#39;Source and destination must not be the same&#39;)

        # TODO: more transfer modes
        match self.mode:
            case &#39;default&#39;:
                return super().execute(context)
            case _:
                raise AirflowFailException(f&#39;Invalid or unsupported transfer mode: {self.mode}&#39;)

def load_table(
        table: Union[str, Table],
        conn_id: Optional[str] = None,
        session: Optional[ETLSession] = None,
        sql: Optional[str] = None) -&gt; XComArg:
    &#34;&#34;&#34; Loads table into memory.

    This is a wrapper over Astro-SDK `run_raw_sql` to
    load data from given database table into XCom and make it available for 
    further processing.

    SQL templating is supported, e.g. if a template for given table was found, it
    will be executed to get the data (see `astro_extras.utils.template.get_template_file`).

    Please note that in order to operate even on modest volumes of data,
    intermediate XCom storage might be required. Easiest way to set it up is to use
    local Parquet file storage by setting
    `AIRFLOW__ASTRO_SDK__XCOM_STORAGE_CONN_ID=local` environment
    variable. However, this will add extra serialization/deserialization
    operation to every task thus increasing overall DAG execution time.

    See https://astro-sdk-python.readthedocs.io/en/1.2.0/guides/xcom_backend.html
    for details.

    Args:
        table:  Either a table name or Astro-SDK `Table` object to load data from
        conn_id:    Airflow connection ID to underlying database. If not specified,
            and `Table` object is passed it, its `conn_id` attribute will be used.
        session:    `astro_extras.operators.session.ETLSession` object. 
            Used only to link up to the `open_session` operator.
        sql:    Custom SQL to load data, used only if no SQL template found.
            If neither SQL nor template is given, all table data will be loaded.

    Results:
        `XComArg` object suitable for further manipulations with Astro-SDK functions

    Examples:
        &gt;&gt;&gt; @aql.dataframe
        &gt;&gt;&gt; def modify_data(data: pd.DataFrame):
        &gt;&gt;&gt;     data[&#39;some_column&#39;] = &#39;new_value&#39;
        &gt;&gt;&gt;     return data
        &gt;&gt;&gt; data = load_table(&#39;test_table&#39;, conn_id=&#39;source_db&#39;)
        &gt;&gt;&gt; modified_data = modify_data(data)
        &gt;&gt;&gt; save_table(modified_data, conn_id=&#39;target_db&#39;)
    &#34;&#34;&#34;

    if not isinstance(table, Table):
        dag = DagContext.get_current_dag()
        @aql.run_raw_sql(handler=lambda result: result.fetchall(),
                        conn_id=conn_id,
                        task_id=f&#39;load-{table}&#39;,
                        results_format=&#39;pandas_dataframe&#39;)
        def _load_table_by_name(table: str, session: ETLSession):
            sql_file = get_template_file(table, &#39;.sql&#39;, dag=dag)
            return sql or sql_file or f&#39;select * from {table}&#39;

        return _load_table_by_name(table, session)
    else:
        dag = DagContext.get_current_dag()
        @aql.run_raw_sql(handler=lambda result: result.fetchall(),
                        conn_id=conn_id,
                        task_id=f&#39;load-{table.name}&#39;,
                        results_format=&#39;pandas_dataframe&#39;)
        def _load_table(table: Table, session: ETLSession):
            sql_file = get_template_file(table.name, &#39;.sql&#39;, dag=dag)
            return sql or sql_file or &#39;&#39;&#39;select * from {{table}}&#39;&#39;&#39;

        return _load_table(table, session)

def save_table(
        data: XComArg,
        table: Union[str, Table],
        conn_id: Optional[str] = None,
        session: Optional[ETLSession] = None,
        fail_if_not_exist: Optional[bool] = True) -&gt; XComArg:
    &#34;&#34;&#34; Saves a table into database &#34;&#34;&#34;

    if isinstance(table, Table):
        task_id = f&#39;save-{table.name}&#39;
    else:
        task_id = f&#39;save-{table}&#39;
        table = ensure_table(table, conn_id)

    conn_id = conn_id or table.conn_id
    @aql.dataframe(if_exists=&#39;append&#39;, conn_id=conn_id, task_id=task_id)
    def _save_data(data: pd.DataFrame, session: ETLSession):
        if fail_if_not_exist:
            db = create_database(conn_id, table)
            if not db.table_exists(table):
                raise AirflowFailException(f&#39;Table {table.name} was not found under {conn_id} connection&#39;)
        session = ensure_session(session)
        if session and &#39;session_id&#39; not in data.columns:
            data.insert(0, &#39;session_id&#39;, session.session_id)
        return data

    return _save_data(data, session, output_table=table)

def transfer_table(
        source: Union[str, Table],
        target: Union[str, Table, None] = None,
        mode: Optional[Literal[&#39;default&#39;, &#39;delta&#39;, &#39;full&#39;]] = &#39;default&#39;,
        source_conn_id: Optional[str] = None,
        destination_conn_id: Optional[str] = None,
        session: Union[XComArg, ETLSession, None] = None,
        **kwargs) -&gt; XComArg:
    
    &#34;&#34;&#34; Cross-database data transfer.

    This function implements cross-database geterogenous data transfer.

    It reads data from source table into memory and then sequentaly inserts 
    each record into the target table. Fields order in the source and target tables 
    must be identical and field types must be compatible, or transfer will fail or produce 
    undesirable results.

    To limit data selection or customize fields, a SQL template could be 
    created for the source table (see `astro_extras.utils.template.get_template_file`).
    The template must ensure fields order and type compatibility with the target table.
    If transfer is running under `astro_extras.operators.session.ETLSession` context, 
    a `session_id` field must also be manually added at proper place.

    For example, if these tables are to participate in transfer:

        create table source_data ( a int, b text );
        create table target_data ( session_id int, b text, a int );

    then, this SQL template might be created as `source_data.sql` file:

        select {{ti.xcom_pull(key=&#34;session&#34;).session_id}} as session_id, b, a 
        from source_data;

    Args:
        source: Either a table name or a `Table` object which would be a data source.
            If a string name is provided, it may contain schema definition denoted by `.`. 
            For `Table` objects, schema must be defined in `Metadata` field,
            otherwise Astro SDK might fall to use its default schema.
            If a SQL template exists for this table name, it will be executed,
            otherwise all table data will be selected.

        target: Either a table name or a `Table` object where data will be saved into.
            If a name is provided, it may contain schema definition denoted by `.`. 
            For `Table` objects, schema must be defined in `Metadata` field,
            otherwise Astro SDK might fall to use its default schema.
            If omitted, `source` argument value is used (this makes sense only
            with string table name and different connections).

        mode: Reserved for furter use

        source_conn_id: Source database Airflow connection.
            Used only with string source table name; for `Table` objects, `conn_id` field is used.
            If omitted and `session` argument is provided, `session.source_conn_id` will be used.

        destination_conn_id: Destination database Airflow connection.
            Used only with string target table name; for `Table` objects, `conn_id` field is used.
            If omitted and `session` argument is provided, `session.destination_conn_id` will be used.

        session:    `ETLSession` object. If set and no SQL template is defined,
            a `session_id` field will be automatically added to selection.

        kwargs:     Any parameters passed to underlying `TableTransfer` operator (e.g. `preoperator`, ...)

    Returns:
        `XComArg` object

    Examples:
        Using `Table` objects (note use of `Metadata` object to specify schemas):

        &gt;&gt;&gt; with DAG(...) as dag:
        &gt;&gt;&gt;     input_table = Table(&#39;table_data&#39;, conn_id=&#39;source_db&#39;, 
        &gt;&gt;&gt;                          metadata=Metadata(schema=&#39;public&#39;))
        &gt;&gt;&gt;     output_table = Table(&#39;table_data&#39;, conn_id=&#39;target_db&#39;, 
        &gt;&gt;&gt;                          metadata=Metadata(schema=&#39;stage&#39;))
        &gt;&gt;&gt;     transfer_table(input_table, output_table)

        Using string table name:

        &gt;&gt;&gt; with DAG(...) as dag, ETLSession(&#39;source_db&#39;, &#39;target_db&#39;) as sess:
        &gt;&gt;&gt;     transfer_table(&#39;public.table_data&#39;, session=sess)

    &#34;&#34;&#34;

    source_table = ensure_table(source, source_conn_id)
    dest_table = ensure_table(target, destination_conn_id) or source_table
    op = TableTransfer(
        source_table=source_table,
        destination_table=dest_table,
        mode=mode,
        session=session,
        **kwargs_with_datasets(kwargs=kwargs, 
                               input_datasets=source_table, 
                               output_datasets=dest_table)
    )
    return XComArg(op)

def declare_tables(
        table_names: Iterable[str],
        conn_id: Optional[str] = None
) -&gt; List[Table]:
    &#34;&#34;&#34; Convert list of string table names to list of `Table` objects &#34;&#34;&#34;

    return [ensure_table(t, conn_id) for t in table_names]

def transfer_tables(
        source_tables: List[Union[str, Table]],
        target_tables: Optional[List[Union[str, Table]]] = None,
        mode: Optional[Literal[&#39;default&#39;, &#39;delta&#39;, &#39;full&#39;]] = &#39;default&#39;,
        source_conn_id: Optional[str] = None,
        destination_conn_id: Optional[str] = None,
        group_id: Optional[str] = None,
        num_parallel: Optional[int] = 1,
        session: Union[XComArg, ETLSession, None] = None,
        **kwargs) -&gt; TaskGroup:
    &#34;&#34;&#34; Transfer multiple tables &#34;&#34;&#34;

    if target_tables and len(target_tables) != len(source_tables):
        raise AirflowFailException(f&#39;Source and target tables list size must be equal&#39;)

    target_tables = target_tables or source_tables

    with TaskGroup(group_id or &#39;transfer-tables&#39;, add_suffix_on_collision=True) as tg:
        ops_list = []
        for (source, target) in zip(source_tables, target_tables):
            source_table = ensure_table(source, source_conn_id)
            dest_table = ensure_table(target, destination_conn_id) or source_table
            op = TableTransfer(
                source_table=source_table,
                destination_table=dest_table,
                mode=mode,
                session=session,
                **kwargs_with_datasets(
                    kwargs=kwargs, 
                    input_datasets=source_table, 
                    output_datasets=dest_table))
            ops_list.append(op)
        schedule_ops(ops_list, num_parallel)
    return tg</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="astro_extras.operators.table.declare_tables"><code class="name flex">
<span>def <span class="ident">declare_tables</span></span>(<span>table_names: Iterable[str], conn_id: Optional[str] = None) ‑> List[astro.table.Table]</span>
</code></dt>
<dd>
<div class="desc"><p>Convert list of string table names to list of <code>Table</code> objects</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def declare_tables(
        table_names: Iterable[str],
        conn_id: Optional[str] = None
) -&gt; List[Table]:
    &#34;&#34;&#34; Convert list of string table names to list of `Table` objects &#34;&#34;&#34;

    return [ensure_table(t, conn_id) for t in table_names]</code></pre>
</details>
</dd>
<dt id="astro_extras.operators.table.load_table"><code class="name flex">
<span>def <span class="ident">load_table</span></span>(<span>table: Union[str, astro.table.Table], conn_id: Optional[str] = None, session: Optional[<a title="astro_extras.operators.session.ETLSession" href="session.html#astro_extras.operators.session.ETLSession">ETLSession</a>] = None, sql: Optional[str] = None) ‑> airflow.models.xcom_arg.XComArg</span>
</code></dt>
<dd>
<div class="desc"><p>Loads table into memory.</p>
<p>This is a wrapper over Astro-SDK <code>run_raw_sql</code> to
load data from given database table into XCom and make it available for
further processing.</p>
<p>SQL templating is supported, e.g. if a template for given table was found, it
will be executed to get the data (see <code><a title="astro_extras.utils.template.get_template_file" href="../utils/template.html#astro_extras.utils.template.get_template_file">get_template_file()</a></code>).</p>
<p>Please note that in order to operate even on modest volumes of data,
intermediate XCom storage might be required. Easiest way to set it up is to use
local Parquet file storage by setting
<code>AIRFLOW__ASTRO_SDK__XCOM_STORAGE_CONN_ID=local</code> environment
variable. However, this will add extra serialization/deserialization
operation to every task thus increasing overall DAG execution time.</p>
<p>See <a href="https://astro-sdk-python.readthedocs.io/en/1.2.0/guides/xcom_backend.html">https://astro-sdk-python.readthedocs.io/en/1.2.0/guides/xcom_backend.html</a>
for details.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>table</code></strong></dt>
<dd>Either a table name or Astro-SDK <code>Table</code> object to load data from</dd>
<dt><strong><code>conn_id</code></strong></dt>
<dd>Airflow connection ID to underlying database. If not specified,
and <code>Table</code> object is passed it, its <code>conn_id</code> attribute will be used.</dd>
<dt><strong><code>session</code></strong></dt>
<dd><code><a title="astro_extras.operators.session.ETLSession" href="session.html#astro_extras.operators.session.ETLSession">ETLSession</a></code> object.
Used only to link up to the <code>open_session</code> operator.</dd>
<dt><strong><code>sql</code></strong></dt>
<dd>Custom SQL to load data, used only if no SQL template found.
If neither SQL nor template is given, all table data will be loaded.</dd>
</dl>
<h2 id="results">Results</h2>
<p><code>XComArg</code> object suitable for further manipulations with Astro-SDK functions</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; @aql.dataframe
&gt;&gt;&gt; def modify_data(data: pd.DataFrame):
&gt;&gt;&gt;     data['some_column'] = 'new_value'
&gt;&gt;&gt;     return data
&gt;&gt;&gt; data = load_table('test_table', conn_id='source_db')
&gt;&gt;&gt; modified_data = modify_data(data)
&gt;&gt;&gt; save_table(modified_data, conn_id='target_db')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_table(
        table: Union[str, Table],
        conn_id: Optional[str] = None,
        session: Optional[ETLSession] = None,
        sql: Optional[str] = None) -&gt; XComArg:
    &#34;&#34;&#34; Loads table into memory.

    This is a wrapper over Astro-SDK `run_raw_sql` to
    load data from given database table into XCom and make it available for 
    further processing.

    SQL templating is supported, e.g. if a template for given table was found, it
    will be executed to get the data (see `astro_extras.utils.template.get_template_file`).

    Please note that in order to operate even on modest volumes of data,
    intermediate XCom storage might be required. Easiest way to set it up is to use
    local Parquet file storage by setting
    `AIRFLOW__ASTRO_SDK__XCOM_STORAGE_CONN_ID=local` environment
    variable. However, this will add extra serialization/deserialization
    operation to every task thus increasing overall DAG execution time.

    See https://astro-sdk-python.readthedocs.io/en/1.2.0/guides/xcom_backend.html
    for details.

    Args:
        table:  Either a table name or Astro-SDK `Table` object to load data from
        conn_id:    Airflow connection ID to underlying database. If not specified,
            and `Table` object is passed it, its `conn_id` attribute will be used.
        session:    `astro_extras.operators.session.ETLSession` object. 
            Used only to link up to the `open_session` operator.
        sql:    Custom SQL to load data, used only if no SQL template found.
            If neither SQL nor template is given, all table data will be loaded.

    Results:
        `XComArg` object suitable for further manipulations with Astro-SDK functions

    Examples:
        &gt;&gt;&gt; @aql.dataframe
        &gt;&gt;&gt; def modify_data(data: pd.DataFrame):
        &gt;&gt;&gt;     data[&#39;some_column&#39;] = &#39;new_value&#39;
        &gt;&gt;&gt;     return data
        &gt;&gt;&gt; data = load_table(&#39;test_table&#39;, conn_id=&#39;source_db&#39;)
        &gt;&gt;&gt; modified_data = modify_data(data)
        &gt;&gt;&gt; save_table(modified_data, conn_id=&#39;target_db&#39;)
    &#34;&#34;&#34;

    if not isinstance(table, Table):
        dag = DagContext.get_current_dag()
        @aql.run_raw_sql(handler=lambda result: result.fetchall(),
                        conn_id=conn_id,
                        task_id=f&#39;load-{table}&#39;,
                        results_format=&#39;pandas_dataframe&#39;)
        def _load_table_by_name(table: str, session: ETLSession):
            sql_file = get_template_file(table, &#39;.sql&#39;, dag=dag)
            return sql or sql_file or f&#39;select * from {table}&#39;

        return _load_table_by_name(table, session)
    else:
        dag = DagContext.get_current_dag()
        @aql.run_raw_sql(handler=lambda result: result.fetchall(),
                        conn_id=conn_id,
                        task_id=f&#39;load-{table.name}&#39;,
                        results_format=&#39;pandas_dataframe&#39;)
        def _load_table(table: Table, session: ETLSession):
            sql_file = get_template_file(table.name, &#39;.sql&#39;, dag=dag)
            return sql or sql_file or &#39;&#39;&#39;select * from {{table}}&#39;&#39;&#39;

        return _load_table(table, session)</code></pre>
</details>
</dd>
<dt id="astro_extras.operators.table.save_table"><code class="name flex">
<span>def <span class="ident">save_table</span></span>(<span>data: airflow.models.xcom_arg.XComArg, table: Union[str, astro.table.Table], conn_id: Optional[str] = None, session: Optional[<a title="astro_extras.operators.session.ETLSession" href="session.html#astro_extras.operators.session.ETLSession">ETLSession</a>] = None, fail_if_not_exist: Optional[bool] = True) ‑> airflow.models.xcom_arg.XComArg</span>
</code></dt>
<dd>
<div class="desc"><p>Saves a table into database</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_table(
        data: XComArg,
        table: Union[str, Table],
        conn_id: Optional[str] = None,
        session: Optional[ETLSession] = None,
        fail_if_not_exist: Optional[bool] = True) -&gt; XComArg:
    &#34;&#34;&#34; Saves a table into database &#34;&#34;&#34;

    if isinstance(table, Table):
        task_id = f&#39;save-{table.name}&#39;
    else:
        task_id = f&#39;save-{table}&#39;
        table = ensure_table(table, conn_id)

    conn_id = conn_id or table.conn_id
    @aql.dataframe(if_exists=&#39;append&#39;, conn_id=conn_id, task_id=task_id)
    def _save_data(data: pd.DataFrame, session: ETLSession):
        if fail_if_not_exist:
            db = create_database(conn_id, table)
            if not db.table_exists(table):
                raise AirflowFailException(f&#39;Table {table.name} was not found under {conn_id} connection&#39;)
        session = ensure_session(session)
        if session and &#39;session_id&#39; not in data.columns:
            data.insert(0, &#39;session_id&#39;, session.session_id)
        return data

    return _save_data(data, session, output_table=table)</code></pre>
</details>
</dd>
<dt id="astro_extras.operators.table.transfer_table"><code class="name flex">
<span>def <span class="ident">transfer_table</span></span>(<span>source: Union[str, astro.table.Table], target: Union[str, astro.table.Table, ForwardRef(None)] = None, mode: Optional[Literal['default', 'delta', 'full']] = 'default', source_conn_id: Optional[str] = None, destination_conn_id: Optional[str] = None, session: Union[<a title="astro_extras.operators.session.ETLSession" href="session.html#astro_extras.operators.session.ETLSession">ETLSession</a>, airflow.models.xcom_arg.XComArg, ForwardRef(None)] = None, **kwargs) ‑> airflow.models.xcom_arg.XComArg</span>
</code></dt>
<dd>
<div class="desc"><p>Cross-database data transfer.</p>
<p>This function implements cross-database geterogenous data transfer.</p>
<p>It reads data from source table into memory and then sequentaly inserts
each record into the target table. Fields order in the source and target tables
must be identical and field types must be compatible, or transfer will fail or produce
undesirable results.</p>
<p>To limit data selection or customize fields, a SQL template could be
created for the source table (see <code><a title="astro_extras.utils.template.get_template_file" href="../utils/template.html#astro_extras.utils.template.get_template_file">get_template_file()</a></code>).
The template must ensure fields order and type compatibility with the target table.
If transfer is running under <code><a title="astro_extras.operators.session.ETLSession" href="session.html#astro_extras.operators.session.ETLSession">ETLSession</a></code> context,
a <code>session_id</code> field must also be manually added at proper place.</p>
<p>For example, if these tables are to participate in transfer:</p>
<pre><code>create table source_data ( a int, b text );
create table target_data ( session_id int, b text, a int );
</code></pre>
<p>then, this SQL template might be created as <code>source_data.sql</code> file:</p>
<pre><code>select {{ti.xcom_pull(key="session").session_id}} as session_id, b, a 
from source_data;
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong></dt>
<dd>Either a table name or a <code>Table</code> object which would be a data source.
If a string name is provided, it may contain schema definition denoted by <code>.</code>.
For <code>Table</code> objects, schema must be defined in <code>Metadata</code> field,
otherwise Astro SDK might fall to use its default schema.
If a SQL template exists for this table name, it will be executed,
otherwise all table data will be selected.</dd>
<dt><strong><code>target</code></strong></dt>
<dd>Either a table name or a <code>Table</code> object where data will be saved into.
If a name is provided, it may contain schema definition denoted by <code>.</code>.
For <code>Table</code> objects, schema must be defined in <code>Metadata</code> field,
otherwise Astro SDK might fall to use its default schema.
If omitted, <code>source</code> argument value is used (this makes sense only
with string table name and different connections).</dd>
<dt><strong><code>mode</code></strong></dt>
<dd>Reserved for furter use</dd>
<dt><strong><code>source_conn_id</code></strong></dt>
<dd>Source database Airflow connection.
Used only with string source table name; for <code>Table</code> objects, <code>conn_id</code> field is used.
If omitted and <code>session</code> argument is provided, <code>session.source_conn_id</code> will be used.</dd>
<dt><strong><code>destination_conn_id</code></strong></dt>
<dd>Destination database Airflow connection.
Used only with string target table name; for <code>Table</code> objects, <code>conn_id</code> field is used.
If omitted and <code>session</code> argument is provided, <code>session.destination_conn_id</code> will be used.</dd>
<dt><strong><code>session</code></strong></dt>
<dd><code>ETLSession</code> object. If set and no SQL template is defined,
a <code>session_id</code> field will be automatically added to selection.</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>
<p>Any parameters passed to underlying <code><a title="astro_extras.operators.table.TableTransfer" href="#astro_extras.operators.table.TableTransfer">TableTransfer</a></code> operator (e.g. <code>preoperator</code>, &hellip;)</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>XComArg</code> object</p>
<h2 id="examples">Examples</h2>
<p>Using <code>Table</code> objects (note use of <code>Metadata</code> object to specify schemas):</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; with DAG(...) as dag:
&gt;&gt;&gt;     input_table = Table('table_data', conn_id='source_db', 
&gt;&gt;&gt;                          metadata=Metadata(schema='public'))
&gt;&gt;&gt;     output_table = Table('table_data', conn_id='target_db', 
&gt;&gt;&gt;                          metadata=Metadata(schema='stage'))
&gt;&gt;&gt;     transfer_table(input_table, output_table)
</code></pre>
<p>Using string table name:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; with DAG(...) as dag, ETLSession('source_db', 'target_db') as sess:
&gt;&gt;&gt;     transfer_table('public.table_data', session=sess)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transfer_table(
        source: Union[str, Table],
        target: Union[str, Table, None] = None,
        mode: Optional[Literal[&#39;default&#39;, &#39;delta&#39;, &#39;full&#39;]] = &#39;default&#39;,
        source_conn_id: Optional[str] = None,
        destination_conn_id: Optional[str] = None,
        session: Union[XComArg, ETLSession, None] = None,
        **kwargs) -&gt; XComArg:
    
    &#34;&#34;&#34; Cross-database data transfer.

    This function implements cross-database geterogenous data transfer.

    It reads data from source table into memory and then sequentaly inserts 
    each record into the target table. Fields order in the source and target tables 
    must be identical and field types must be compatible, or transfer will fail or produce 
    undesirable results.

    To limit data selection or customize fields, a SQL template could be 
    created for the source table (see `astro_extras.utils.template.get_template_file`).
    The template must ensure fields order and type compatibility with the target table.
    If transfer is running under `astro_extras.operators.session.ETLSession` context, 
    a `session_id` field must also be manually added at proper place.

    For example, if these tables are to participate in transfer:

        create table source_data ( a int, b text );
        create table target_data ( session_id int, b text, a int );

    then, this SQL template might be created as `source_data.sql` file:

        select {{ti.xcom_pull(key=&#34;session&#34;).session_id}} as session_id, b, a 
        from source_data;

    Args:
        source: Either a table name or a `Table` object which would be a data source.
            If a string name is provided, it may contain schema definition denoted by `.`. 
            For `Table` objects, schema must be defined in `Metadata` field,
            otherwise Astro SDK might fall to use its default schema.
            If a SQL template exists for this table name, it will be executed,
            otherwise all table data will be selected.

        target: Either a table name or a `Table` object where data will be saved into.
            If a name is provided, it may contain schema definition denoted by `.`. 
            For `Table` objects, schema must be defined in `Metadata` field,
            otherwise Astro SDK might fall to use its default schema.
            If omitted, `source` argument value is used (this makes sense only
            with string table name and different connections).

        mode: Reserved for furter use

        source_conn_id: Source database Airflow connection.
            Used only with string source table name; for `Table` objects, `conn_id` field is used.
            If omitted and `session` argument is provided, `session.source_conn_id` will be used.

        destination_conn_id: Destination database Airflow connection.
            Used only with string target table name; for `Table` objects, `conn_id` field is used.
            If omitted and `session` argument is provided, `session.destination_conn_id` will be used.

        session:    `ETLSession` object. If set and no SQL template is defined,
            a `session_id` field will be automatically added to selection.

        kwargs:     Any parameters passed to underlying `TableTransfer` operator (e.g. `preoperator`, ...)

    Returns:
        `XComArg` object

    Examples:
        Using `Table` objects (note use of `Metadata` object to specify schemas):

        &gt;&gt;&gt; with DAG(...) as dag:
        &gt;&gt;&gt;     input_table = Table(&#39;table_data&#39;, conn_id=&#39;source_db&#39;, 
        &gt;&gt;&gt;                          metadata=Metadata(schema=&#39;public&#39;))
        &gt;&gt;&gt;     output_table = Table(&#39;table_data&#39;, conn_id=&#39;target_db&#39;, 
        &gt;&gt;&gt;                          metadata=Metadata(schema=&#39;stage&#39;))
        &gt;&gt;&gt;     transfer_table(input_table, output_table)

        Using string table name:

        &gt;&gt;&gt; with DAG(...) as dag, ETLSession(&#39;source_db&#39;, &#39;target_db&#39;) as sess:
        &gt;&gt;&gt;     transfer_table(&#39;public.table_data&#39;, session=sess)

    &#34;&#34;&#34;

    source_table = ensure_table(source, source_conn_id)
    dest_table = ensure_table(target, destination_conn_id) or source_table
    op = TableTransfer(
        source_table=source_table,
        destination_table=dest_table,
        mode=mode,
        session=session,
        **kwargs_with_datasets(kwargs=kwargs, 
                               input_datasets=source_table, 
                               output_datasets=dest_table)
    )
    return XComArg(op)</code></pre>
</details>
</dd>
<dt id="astro_extras.operators.table.transfer_tables"><code class="name flex">
<span>def <span class="ident">transfer_tables</span></span>(<span>source_tables: List[Union[str, astro.table.Table]], target_tables: Optional[List[Union[str, astro.table.Table]]] = None, mode: Optional[Literal['default', 'delta', 'full']] = 'default', source_conn_id: Optional[str] = None, destination_conn_id: Optional[str] = None, group_id: Optional[str] = None, num_parallel: Optional[int] = 1, session: Union[<a title="astro_extras.operators.session.ETLSession" href="session.html#astro_extras.operators.session.ETLSession">ETLSession</a>, airflow.models.xcom_arg.XComArg, ForwardRef(None)] = None, **kwargs) ‑> airflow.utils.task_group.TaskGroup</span>
</code></dt>
<dd>
<div class="desc"><p>Transfer multiple tables</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transfer_tables(
        source_tables: List[Union[str, Table]],
        target_tables: Optional[List[Union[str, Table]]] = None,
        mode: Optional[Literal[&#39;default&#39;, &#39;delta&#39;, &#39;full&#39;]] = &#39;default&#39;,
        source_conn_id: Optional[str] = None,
        destination_conn_id: Optional[str] = None,
        group_id: Optional[str] = None,
        num_parallel: Optional[int] = 1,
        session: Union[XComArg, ETLSession, None] = None,
        **kwargs) -&gt; TaskGroup:
    &#34;&#34;&#34; Transfer multiple tables &#34;&#34;&#34;

    if target_tables and len(target_tables) != len(source_tables):
        raise AirflowFailException(f&#39;Source and target tables list size must be equal&#39;)

    target_tables = target_tables or source_tables

    with TaskGroup(group_id or &#39;transfer-tables&#39;, add_suffix_on_collision=True) as tg:
        ops_list = []
        for (source, target) in zip(source_tables, target_tables):
            source_table = ensure_table(source, source_conn_id)
            dest_table = ensure_table(target, destination_conn_id) or source_table
            op = TableTransfer(
                source_table=source_table,
                destination_table=dest_table,
                mode=mode,
                session=session,
                **kwargs_with_datasets(
                    kwargs=kwargs, 
                    input_datasets=source_table, 
                    output_datasets=dest_table))
            ops_list.append(op)
        schedule_ops(ops_list, num_parallel)
    return tg</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="astro_extras.operators.table.TableTransfer"><code class="flex name class">
<span>class <span class="ident">TableTransfer</span></span>
<span>(</span><span>*, source_table: astro.table.Table, destination_table: astro.table.Table, mode: Optional[Literal['default', 'delta', 'full']] = 'default', session: Optional[<a title="astro_extras.operators.session.ETLSession" href="session.html#astro_extras.operators.session.ETLSession">ETLSession</a>] = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Customized table transfer operator. Usually is used within <code><a title="astro_extras.operators.table.transfer_table" href="#astro_extras.operators.table.transfer_table">transfer_table()</a></code> function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TableTransfer(GenericTransfer):
    &#34;&#34;&#34; Customized table transfer operator. Usually is used within `transfer_table` function &#34;&#34;&#34;

    template_fields = (&#34;sql&#34;, &#34;preoperator&#34;, &#34;source_table&#34;, &#34;destination_table&#34;)
    template_ext = (&#34;.sql&#34;, &#34;.hql&#34; )
    template_fields_renderers = {&#34;sql&#34;: &#34;sql&#34;, &#34;preoperator&#34;: &#34;sql&#34;}
    ui_color = &#34;#b0f07c&#34;

    def __init__(
        self,
        *,
        source_table: Table,
        destination_table: Table,
        mode: Optional[Literal[&#39;default&#39;, &#39;delta&#39;, &#39;full&#39;]] = &#39;default&#39;,
        session: Optional[ETLSession] = None,
        **kwargs,
    ) -&gt; None:

        source_db = create_database(source_table.conn_id, source_table)
        dest_db = create_database(destination_table.conn_id, destination_table)

        task_id = kwargs.pop(&#39;task_id&#39;, f&#39;transfer-{source_table.name}&#39;)
        sql = kwargs.pop(&#39;sql&#39;, self._get_sql(source_table, source_db, session))

        super().__init__(task_id=task_id,
                         sql=sql,
                         destination_table=dest_db.get_table_qualified_name(destination_table),
                         source_conn_id=source_table.conn_id,
                         destination_conn_id=destination_table.conn_id,
                         **kwargs)

        self.source = source_table
        self.source_table = source_db.get_table_qualified_name(source_table)
        self.destination = destination_table
        self.mode = mode
        self.session = session

    def _get_sql(self, table: Table, db: BaseDatabase, session: ETLSession) -&gt; str:
        &#34;&#34;&#34; Internal - get a sql statement or template for given table &#34;&#34;&#34;

        if (sql_file := get_template_file(table.name, &#39;.sql&#39;)):
            self.log.info(f&#39;Using template file {sql_file}&#39;)
            return sql_file

        full_name = db.get_table_qualified_name(table)
        if session:
            return &#39;select {{ti.xcom_pull(key=&#34;session&#34;).session_id}} as session_id, * from &#39; + full_name

        return &#39;select * from &#39; + full_name

    def execute(self, context: Context):
        self.session = ensure_session(self.session, context)
        if self.session:
            if not self.source_conn_id:
                self.source_conn_id = self.session.source_conn_id
            if not self.destination_conn_id:
                self.destination_conn_id = self.session.destination_conn_id

        if not self.source_conn_id:
            raise AirflowFailException(&#39;source connection not specified&#39;)
        if not self.destination_conn_id:
            raise AirflowFailException(&#39;destination connection not specified&#39;)
        if self.source_conn_id == self.destination_conn_id and self.source_table == self.destination_table:
            raise AirflowFailException(&#39;Source and destination must not be the same&#39;)

        # TODO: more transfer modes
        match self.mode:
            case &#39;default&#39;:
                return super().execute(context)
            case _:
                raise AirflowFailException(f&#39;Invalid or unsupported transfer mode: {self.mode}&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>airflow.operators.generic_transfer.GenericTransfer</li>
<li>airflow.models.baseoperator.BaseOperator</li>
<li>airflow.models.abstractoperator.AbstractOperator</li>
<li>airflow.template.templater.Templater</li>
<li>airflow.utils.log.logging_mixin.LoggingMixin</li>
<li>airflow.models.taskmixin.DAGNode</li>
<li>airflow.models.taskmixin.DependencyMixin</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="astro_extras.operators.table.TableTransfer.template_ext"><code class="name">var <span class="ident">template_ext</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="astro_extras.operators.table.TableTransfer.template_fields"><code class="name">var <span class="ident">template_fields</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="astro_extras.operators.table.TableTransfer.template_fields_renderers"><code class="name">var <span class="ident">template_fields_renderers</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="astro_extras.operators.table.TableTransfer.ui_color"><code class="name">var <span class="ident">ui_color</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="astro_extras.operators.table.TableTransfer.execute"><code class="name flex">
<span>def <span class="ident">execute</span></span>(<span>self, context: airflow.utils.context.Context)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the main method to derive when creating an operator.
Context is the same dictionary used as when rendering jinja templates.</p>
<p>Refer to get_template_context for more context.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute(self, context: Context):
    self.session = ensure_session(self.session, context)
    if self.session:
        if not self.source_conn_id:
            self.source_conn_id = self.session.source_conn_id
        if not self.destination_conn_id:
            self.destination_conn_id = self.session.destination_conn_id

    if not self.source_conn_id:
        raise AirflowFailException(&#39;source connection not specified&#39;)
    if not self.destination_conn_id:
        raise AirflowFailException(&#39;destination connection not specified&#39;)
    if self.source_conn_id == self.destination_conn_id and self.source_table == self.destination_table:
        raise AirflowFailException(&#39;Source and destination must not be the same&#39;)

    # TODO: more transfer modes
    match self.mode:
        case &#39;default&#39;:
            return super().execute(context)
        case _:
            raise AirflowFailException(f&#39;Invalid or unsupported transfer mode: {self.mode}&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="astro_extras.operators" href="index.html">astro_extras.operators</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="astro_extras.operators.table.declare_tables" href="#astro_extras.operators.table.declare_tables">declare_tables</a></code></li>
<li><code><a title="astro_extras.operators.table.load_table" href="#astro_extras.operators.table.load_table">load_table</a></code></li>
<li><code><a title="astro_extras.operators.table.save_table" href="#astro_extras.operators.table.save_table">save_table</a></code></li>
<li><code><a title="astro_extras.operators.table.transfer_table" href="#astro_extras.operators.table.transfer_table">transfer_table</a></code></li>
<li><code><a title="astro_extras.operators.table.transfer_tables" href="#astro_extras.operators.table.transfer_tables">transfer_tables</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="astro_extras.operators.table.TableTransfer" href="#astro_extras.operators.table.TableTransfer">TableTransfer</a></code></h4>
<ul class="">
<li><code><a title="astro_extras.operators.table.TableTransfer.execute" href="#astro_extras.operators.table.TableTransfer.execute">execute</a></code></li>
<li><code><a title="astro_extras.operators.table.TableTransfer.template_ext" href="#astro_extras.operators.table.TableTransfer.template_ext">template_ext</a></code></li>
<li><code><a title="astro_extras.operators.table.TableTransfer.template_fields" href="#astro_extras.operators.table.TableTransfer.template_fields">template_fields</a></code></li>
<li><code><a title="astro_extras.operators.table.TableTransfer.template_fields_renderers" href="#astro_extras.operators.table.TableTransfer.template_fields_renderers">template_fields_renderers</a></code></li>
<li><code><a title="astro_extras.operators.table.TableTransfer.ui_color" href="#astro_extras.operators.table.TableTransfer.ui_color">ui_color</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>